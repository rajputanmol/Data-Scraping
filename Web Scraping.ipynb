{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a44e86b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"From today's featured article\",\n",
       " 'Did you know\\xa0...',\n",
       " 'In the news',\n",
       " 'On this day',\n",
       " \"Today's featured picture\",\n",
       " 'Other areas of Wikipedia',\n",
       " \"Wikipedia's sister projects\",\n",
       " 'Wikipedia languages']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "# coding: utf-8\n",
    "\n",
    "# # Ques1: Write a python program to display all the header tags from wikipedia.org.\n",
    "\n",
    "\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "\n",
    "\n",
    "\n",
    "page=requests.get('https://en.wikipedia.org/wiki/Main_Page')\n",
    "\n",
    "\n",
    "\n",
    "soup=BeautifulSoup(page.content)\n",
    "soup\n",
    "\n",
    "\n",
    "\n",
    "titles =[]\n",
    "\n",
    "for i in soup.find_all('h2', class_=\"mp-h2\"):\n",
    "    titles.append(i.text)\n",
    "\n",
    "titles\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "91ea0b62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 100 100\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Names</th>\n",
       "      <th>IMDB Ratings</th>\n",
       "      <th>Year of Release</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[, 1., The Shawshank Redemption, (1994), ]</td>\n",
       "      <td>9.3</td>\n",
       "      <td>(1994)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[, 2., The Godfather, (1972), ]</td>\n",
       "      <td>9.2</td>\n",
       "      <td>(1972)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[, 3., The Dark Knight, (2008), ]</td>\n",
       "      <td>9.0</td>\n",
       "      <td>(2008)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[, 4., The Lord of the Rings: The Return of th...</td>\n",
       "      <td>9.0</td>\n",
       "      <td>(2003)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[, 5., Schindler's List, (1993), ]</td>\n",
       "      <td>9.0</td>\n",
       "      <td>(1993)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>[, 96., North by Northwest, (1959), ]</td>\n",
       "      <td>8.3</td>\n",
       "      <td>(1959)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>[, 97., Vertigo, (1958), ]</td>\n",
       "      <td>8.3</td>\n",
       "      <td>(1958)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>[, 98., Singin' in the Rain, (1952), ]</td>\n",
       "      <td>8.3</td>\n",
       "      <td>(1952)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>[, 99., Citizen Kane, (1941), ]</td>\n",
       "      <td>8.3</td>\n",
       "      <td>(1941)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>[, 100., M - Eine Stadt sucht einen Mörder, (1...</td>\n",
       "      <td>8.3</td>\n",
       "      <td>(1931)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Names IMDB Ratings  \\\n",
       "0          [, 1., The Shawshank Redemption, (1994), ]          9.3   \n",
       "1                     [, 2., The Godfather, (1972), ]          9.2   \n",
       "2                   [, 3., The Dark Knight, (2008), ]          9.0   \n",
       "3   [, 4., The Lord of the Rings: The Return of th...          9.0   \n",
       "4                  [, 5., Schindler's List, (1993), ]          9.0   \n",
       "..                                                ...          ...   \n",
       "95              [, 96., North by Northwest, (1959), ]          8.3   \n",
       "96                         [, 97., Vertigo, (1958), ]          8.3   \n",
       "97             [, 98., Singin' in the Rain, (1952), ]          8.3   \n",
       "98                    [, 99., Citizen Kane, (1941), ]          8.3   \n",
       "99  [, 100., M - Eine Stadt sucht einen Mörder, (1...          8.3   \n",
       "\n",
       "   Year of Release  \n",
       "0           (1994)  \n",
       "1           (1972)  \n",
       "2           (2008)  \n",
       "3           (2003)  \n",
       "4           (1993)  \n",
       "..             ...  \n",
       "95          (1959)  \n",
       "96          (1958)  \n",
       "97          (1952)  \n",
       "98          (1941)  \n",
       "99          (1931)  \n",
       "\n",
       "[100 rows x 3 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# # Ques2: Write a python program to display IMDB’s Top rated 100 movies’ data (i.e. name, rating, year of release) and make data frame.\n",
    "\n",
    "\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "\n",
    "\n",
    "\n",
    "page_1=requests.get('https://www.imdb.com/search/title/?groups=top_100&sort=user_rating,desc&ref_=adv_prv')\n",
    "\n",
    "\n",
    "\n",
    "page_1\n",
    "\n",
    "\n",
    "\n",
    "soup_1=BeautifulSoup(page_1.content)\n",
    "soup_1\n",
    "\n",
    "\n",
    "\n",
    "#Importing names of first 50 IMDB rating movies available on Page 1\n",
    "\n",
    "movies_name=[]\n",
    "\n",
    "for i in soup_1.find_all('h3', class_=\"lister-item-header\"):\n",
    "    movies_name.append(i.text.split(sep='\\n'))\n",
    "    \n",
    "movies_name\n",
    "\n",
    "\n",
    "\n",
    "ratings=[]\n",
    "\n",
    "for i in soup_1.find_all('div', class_=\"inline-block ratings-imdb-rating\"):\n",
    "    ratings.append(i.text.split()[0])\n",
    "    \n",
    "ratings\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#Year of Release Page 1\n",
    "\n",
    "yor=[]\n",
    "\n",
    "for i in soup_1.find_all('span', class_=\"lister-item-year text-muted unbold\"):\n",
    "    yor.append(i.text)\n",
    "\n",
    "yor\n",
    "\n",
    "\n",
    "# Page 2\n",
    "\n",
    "\n",
    "\n",
    "page_2=requests.get('https://www.imdb.com/search/title/?groups=top_100&sort=user_rating,desc&start=51&ref_=adv_nxt')\n",
    "page_2\n",
    "\n",
    "\n",
    "\n",
    "soup_2=BeautifulSoup(page_2.content)\n",
    "soup_2\n",
    "\n",
    "\n",
    "\n",
    "for i in soup_2.find_all('h3', class_=\"lister-item-header\"):\n",
    "    movies_name.append(i.text.split(sep='\\n'))\n",
    "    \n",
    "movies_name\n",
    "\n",
    "\n",
    "\n",
    "#Ratings Page 2\n",
    "\n",
    "for i in soup_2.find_all('div', class_=\"inline-block ratings-imdb-rating\"):\n",
    "    ratings.append(i.text.split()[0])\n",
    "    \n",
    "ratings\n",
    "\n",
    "\n",
    "\n",
    "#Year of Release Page 2\n",
    "\n",
    "for i in soup_2.find_all('span', class_=\"lister-item-year text-muted unbold\"):\n",
    "    yor.append(i.text)\n",
    "\n",
    "yor\n",
    "\n",
    "\n",
    "\n",
    "#Checking length\n",
    "print(len(movies_name),len(ratings), len(yor))\n",
    "\n",
    "\n",
    "\n",
    "#Creating DataFrame\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "imdb = pd.DataFrame({'Names': movies_name,'IMDB Ratings':ratings, 'Year of Release': yor})\n",
    "imdb\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d3209f26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['', '      1.', '      Jai Bhim', '(2021)', '']\n",
      "['', '      2.', '      Anbe Sivam', '(2003)', '']\n",
      "['', '      3.', '      Golmaal', '(1979)', '']\n",
      "['', '      4.', '      Nayakan', '(1987)', '']\n",
      "['', '      5.', '      Pariyerum Perumal', '(2018)', '']\n",
      "['', '      6.', '      Apur Sansar', '(1959)', '']\n",
      "['', '      7.', '      3 Idiots', '(2009)', '']\n",
      "['', '      8.', '      Manichitrathazhu', '(1993)', '']\n",
      "['', '      9.', '      Black Friday', '(2004)', '']\n",
      "['', '      10.', '      Kumbalangi Nights', '(2019)', '']\n",
      "['', '      11.', '      C/o Kancharapalem', '(2018)', '']\n",
      "['', '      12.', '      Taare Zameen Par', '(2007)', '']\n",
      "['', '      13.', '      #Home', '(2021)', '']\n",
      "['', '      14.', '      Kireedam', '(1989)', '']\n",
      "['', '      15.', '      Soorarai Pottru', '(2020)', '']\n",
      "['', '      16.', '      Dangal', '(2016)', '']\n",
      "['', '      17.', '      Jersey', '(2019)', '']\n",
      "['', '      18.', '      Kaithi', '(2019)', '']\n",
      "['', '      19.', '      Pather Panchali', '(1955)', '']\n",
      "['', '      20.', '      Thevar Magan', '(1992)', '']\n",
      "['', '      21.', '      Asuran', '(2019)', '']\n",
      "['', '      22.', '      Visaaranai', '(2015)', '']\n",
      "['', '      23.', '      96', '(2018)', '']\n",
      "['', '      24.', '      Thalapathi', '(1991)', '']\n",
      "['', '      25.', '      Natsamrat', '(2016)', '']\n",
      "['', '      26.', '      Sarpatta Parambarai', '(2021)', '']\n",
      "['', '      27.', '      Drishyam 2', '(2021)', '']\n",
      "['', '      28.', '      Sardar Udham', '(2021)', '']\n",
      "['', '      29.', '      Thani Oruvan', '(2015)', '']\n",
      "['', '      30.', '      Aparajito', '(1956)', '']\n",
      "['', '      31.', '      Vada Chennai', '(2018)', '']\n",
      "['', '      32.', '      Jaane Bhi Do Yaaro', '(1983)', '']\n",
      "['', '      33.', '      Khosla Ka Ghosla!', '(2006)', '']\n",
      "['', '      34.', '      Peranbu', '(2018)', '']\n",
      "['', '      35.', '      K.G.F: Chapter 2', '(2022)', '']\n",
      "['', '      36.', '      Chupke Chupke', '(1975)', '']\n",
      "['', '      37.', '      Drishyam', '(2013)', '']\n",
      "['', '      38.', '      Agent Sai Srinivasa Athreya', '(2019)', '']\n",
      "['', '      39.', '      Anniyan', '(2005)', '']\n",
      "['', '      40.', '      Mahanati', '(2018)', '']\n",
      "['', '      41.', '      Satya', '(1998)', '']\n",
      "['', '      42.', '      Bangalore Days', '(2014)', '']\n",
      "['', '      43.', '      Super Deluxe', '(2019)', '']\n",
      "['', '      44.', '      Premam', '(2015)', '']\n",
      "['', '      45.', '      Ratsasan', '(2018)', '']\n",
      "['', '      46.', '      Devasuram', '(1993)', '']\n",
      "['', '      47.', '      Aruvi', '(2016)', '']\n",
      "['', '      48.', '      Bhaag Milkha Bhaag', '(2013)', '']\n",
      "['', '      49.', '      Gangs of Wasseypur', '(2012)', '']\n",
      "['', '      50.', '      Kannathil Muthamittal', '(2002)', '']\n",
      "['', '      51.', '      Andhadhun', '(2018)', '']\n",
      "['', '      52.', '      Guide', '(1965)', '']\n",
      "['', '      53.', '      Drishyam', '(2015)', '']\n",
      "['', '      54.', '      Chithram', '(1988)', '']\n",
      "['', '      55.', '      Iruvar', '(1997)', '']\n",
      "['', '      56.', '      Shahid', '(2012)', '']\n",
      "['', '      57.', '      Sairat', '(2016)', '']\n",
      "['', '      58.', '      Paan Singh Tomar', '(2012)', '']\n",
      "['', '      59.', '      Zindagi Na Milegi Dobara', '(2011)', '']\n",
      "['', '      60.', '      Vikram Vedha', '(2017)', '']\n",
      "['', '      61.', '      Tumbbad', '(2018)', '']\n",
      "['', '      62.', '      Mudhalvan', '(1999)', '']\n",
      "['', '      63.', '      Spadikam', '(1995)', '']\n",
      "['', '      64.', '      Dhuruvangal Pathinaaru', '(2016)', '']\n",
      "['', '      65.', '      Black', '(2005)', '']\n",
      "['', '      66.', '      Swades: We, the People', '(2004)', '']\n",
      "['', '      67.', '      Chhichhore', '(2019)', '']\n",
      "['', '      68.', '      Jo Jeeta Wohi Sikandar', '(1992)', '']\n",
      "['', '      69.', '      Pudhu Pettai', '(2006)', '']\n",
      "['', '      70.', '      Chak De! India', '(2007)', '']\n",
      "['', '      71.', '      Pyaasa', '(1957)', '']\n",
      "['', '      72.', '      Papanasam', '(2015)', '']\n",
      "['', '      73.', '      Soodhu Kavvum', '(2013)', '']\n",
      "['', '      74.', '      Article 15', '(2019)', '']\n",
      "['', '      75.', '      PK', '(2014)', '']\n",
      "['', '      76.', '      Mandela', '(2021)', '']\n",
      "['', '      77.', '      Queen', '(2013)', '']\n",
      "['', '      78.', '      Munna Bhai M.B.B.S.', '(2003)', '']\n",
      "['', '      79.', '      Talvar', '(2015)', '']\n",
      "['', '      80.', '      Kaakkaa Muttai', '(2014)', '']\n",
      "['', '      81.', '      Uri: The Surgical Strike', '(2019)', '']\n",
      "['', '      82.', '      OMG: Oh My God!', '(2012)', '']\n",
      "['', '      83.', '      Lagaan: Once Upon a Time in India', '(2001)', '']\n",
      "['', '      84.', '      Jigarthanda', '(2014)', '']\n",
      "['', '      85.', '      Sarfarosh', '(1999)', '']\n",
      "['', '      86.', '      Udaan', '(2010)', '']\n",
      "['', '      87.', '      Barfi!', '(2012)', '']\n",
      "['', '      88.', '      Theeran Adhigaaram Ondru', '(2017)', '']\n",
      "['', '      89.', '      Sholay', '(1975)', '']\n",
      "['', '      90.', '      Ustad Hotel', '(2012)', '']\n",
      "['', '      91.', '      Hera Pheri', '(2000)', '']\n",
      "['', '      92.', '      The Legend of Bhagat Singh', '(2002)', '']\n",
      "['', '      93.', '      Angoor', '(1982)', '']\n",
      "['', '      94.', '      Baasha', '(1995)', '']\n",
      "['', '      95.', '      Rang De Basanti', '(2006)', '']\n",
      "['', '      96.', '      Masaan', '(2015)', '']\n",
      "['', '      97.', '      Dil Chahta Hai', '(2001)', '']\n",
      "['', '      98.', '      Kahaani', '(2012)', '']\n",
      "['', '      99.', '      Maheshinte Prathikaaram', '(2016)', '']\n",
      "['', '      100.', '      Baahubali 2: The Conclusion', '(2017)', '']\n",
      "250 250 250\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Names</th>\n",
       "      <th>IMDB Ratings</th>\n",
       "      <th>Year of Release</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[,       1.,       Jai Bhim, (2021), ]</td>\n",
       "      <td>8.4</td>\n",
       "      <td>(2021)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[,       2.,       Anbe Sivam, (2003), ]</td>\n",
       "      <td>8.4</td>\n",
       "      <td>(2003)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[,       3.,       Golmaal, (1979), ]</td>\n",
       "      <td>8.4</td>\n",
       "      <td>(1979)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[,       4.,       Nayakan, (1987), ]</td>\n",
       "      <td>8.4</td>\n",
       "      <td>(1987)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[,       5.,       Pariyerum Perumal, (2018), ]</td>\n",
       "      <td>8.4</td>\n",
       "      <td>(2018)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>245</th>\n",
       "      <td>[,       246.,       Oye Lucky! Lucky Oye!, (2...</td>\n",
       "      <td>7.6</td>\n",
       "      <td>(2008)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>246</th>\n",
       "      <td>[,       247.,       Arya, (2004), ]</td>\n",
       "      <td>7.6</td>\n",
       "      <td>(2004)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>247</th>\n",
       "      <td>[,       248.,       Samsara, (2001), ]</td>\n",
       "      <td>7.6</td>\n",
       "      <td>(2001)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>248</th>\n",
       "      <td>[,       249.,       Eega, (2012), ]</td>\n",
       "      <td>7.6</td>\n",
       "      <td>(2012)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>249</th>\n",
       "      <td>[,       250.,       Newton, (2017), ]</td>\n",
       "      <td>7.6</td>\n",
       "      <td>(2017)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>250 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 Names IMDB Ratings  \\\n",
       "0               [,       1.,       Jai Bhim, (2021), ]          8.4   \n",
       "1             [,       2.,       Anbe Sivam, (2003), ]          8.4   \n",
       "2                [,       3.,       Golmaal, (1979), ]          8.4   \n",
       "3                [,       4.,       Nayakan, (1987), ]          8.4   \n",
       "4      [,       5.,       Pariyerum Perumal, (2018), ]          8.4   \n",
       "..                                                 ...          ...   \n",
       "245  [,       246.,       Oye Lucky! Lucky Oye!, (2...          7.6   \n",
       "246               [,       247.,       Arya, (2004), ]          7.6   \n",
       "247            [,       248.,       Samsara, (2001), ]          7.6   \n",
       "248               [,       249.,       Eega, (2012), ]          7.6   \n",
       "249             [,       250.,       Newton, (2017), ]          7.6   \n",
       "\n",
       "    Year of Release  \n",
       "0            (2021)  \n",
       "1            (2003)  \n",
       "2            (1979)  \n",
       "3            (1987)  \n",
       "4            (2018)  \n",
       "..              ...  \n",
       "245          (2008)  \n",
       "246          (2004)  \n",
       "247          (2001)  \n",
       "248          (2012)  \n",
       "249          (2017)  \n",
       "\n",
       "[250 rows x 3 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# # Ques3: Write a python program to display IMDB’s Top rated 100 Indian movies’ data (i.e. name, rating, year of release) and make data frame.\n",
    "\n",
    "\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "\n",
    "\n",
    "\n",
    "page=requests.get('https://www.imdb.com/india/top-rated-indian-movies')\n",
    "\n",
    "\n",
    "\n",
    "soup=BeautifulSoup(page.content)\n",
    "soup\n",
    "\n",
    "\n",
    "\n",
    "n=100\n",
    "movies_name=[] * n\n",
    "\n",
    "for i in soup.find_all('td', class_=\"titleColumn\"):\n",
    "    movies_name.append(i.text.split(sep='\\n')) \n",
    "    \n",
    "for i in range (0,100):\n",
    "    print(movies_name[i])\n",
    "\n",
    "\n",
    "\n",
    "ratings=[]\n",
    "\n",
    "for i in soup.find_all('td', class_=\"ratingColumn imdbRating\"):\n",
    "    ratings.append(i.text.split()[0])\n",
    "    \n",
    "ratings\n",
    "\n",
    "\n",
    "\n",
    "#Year of Release\n",
    "\n",
    "yor=[]\n",
    "\n",
    "for i in soup.find_all('span', class_=\"secondaryInfo\"):\n",
    "    yor.append(i.text)\n",
    "\n",
    "yor\n",
    "\n",
    "\n",
    "\n",
    "#Checking length\n",
    "print(len(movies_name),len(ratings), len(yor))\n",
    "\n",
    "\n",
    "\n",
    "#Creating DataFrame\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "imdb = pd.DataFrame({'Names': movies_name,'IMDB Ratings':ratings, 'Year of Release': yor})\n",
    "imdb\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1a3e635c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13 13\n",
      "25 25 25 25\n",
      "100 100 100 100\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Rank</th>\n",
       "      <th>Publication</th>\n",
       "      <th>H5-index</th>\n",
       "      <th>H5-median</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.</td>\n",
       "      <td>Nature</td>\n",
       "      <td>[414]</td>\n",
       "      <td>607</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2.</td>\n",
       "      <td>The New England Journal of Medicine</td>\n",
       "      <td>[410]</td>\n",
       "      <td>704</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3.</td>\n",
       "      <td>Science</td>\n",
       "      <td>[391]</td>\n",
       "      <td>564</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.</td>\n",
       "      <td>IEEE/CVF Conference on Computer Vision and Pat...</td>\n",
       "      <td>[356]</td>\n",
       "      <td>583</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.</td>\n",
       "      <td>The Lancet</td>\n",
       "      <td>[345]</td>\n",
       "      <td>600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>96.</td>\n",
       "      <td>Frontiers in Immunology</td>\n",
       "      <td>[134]</td>\n",
       "      <td>177</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>97.</td>\n",
       "      <td>Small</td>\n",
       "      <td>[134]</td>\n",
       "      <td>173</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>98.</td>\n",
       "      <td>Nature Immunology</td>\n",
       "      <td>[133]</td>\n",
       "      <td>210</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>99.</td>\n",
       "      <td>JAMA Oncology</td>\n",
       "      <td>[133]</td>\n",
       "      <td>202</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>100.</td>\n",
       "      <td>The Lancet Neurology</td>\n",
       "      <td>[133]</td>\n",
       "      <td>200</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    Rank                                        Publication  H5-index  \\\n",
       "0     1.                                             Nature     [414]   \n",
       "1     2.                The New England Journal of Medicine     [410]   \n",
       "2     3.                                            Science     [391]   \n",
       "3     4.  IEEE/CVF Conference on Computer Vision and Pat...     [356]   \n",
       "4     5.                                         The Lancet     [345]   \n",
       "..   ...                                                ...       ...   \n",
       "95   96.                            Frontiers in Immunology     [134]   \n",
       "96   97.                                              Small     [134]   \n",
       "97   98.                                  Nature Immunology     [133]   \n",
       "98   99.                                      JAMA Oncology     [133]   \n",
       "99  100.                               The Lancet Neurology     [133]   \n",
       "\n",
       "    H5-median  \n",
       "0         607  \n",
       "1         704  \n",
       "2         564  \n",
       "3         583  \n",
       "4         600  \n",
       "..        ...  \n",
       "95        177  \n",
       "96        173  \n",
       "97        210  \n",
       "98        202  \n",
       "99        200  \n",
       "\n",
       "[100 rows x 4 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# # Ques 4: Write s python program to display list of respected former presidents of India(i.e. Name , Term of office) from https://presidentofindia.nic.in/former-presidents.htm\n",
    "\n",
    "\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "\n",
    "\n",
    "\n",
    "page=requests.get('https://presidentofindia.nic.in/former-presidents.htm')\n",
    "page\n",
    "\n",
    "\n",
    "\n",
    "soup=BeautifulSoup(page.content)\n",
    "soup\n",
    "\n",
    "\n",
    "\n",
    "names=[]\n",
    "\n",
    "for i in soup.find_all('div', class_=\"presidentListing\"):\n",
    "    names.append(i.text.split(sep='\\n')[1]) \n",
    "    \n",
    "names\n",
    "\n",
    "\n",
    "\n",
    "term=[]\n",
    "\n",
    "for i in soup.find_all('div', class_=\"presidentListing\"):\n",
    "    term.append(i.text.split(sep='\\n')[2]) \n",
    "    \n",
    "term\n",
    "\n",
    "\n",
    "\n",
    "#Checking length\n",
    "print(len(names),len(term))\n",
    "\n",
    "\n",
    "\n",
    "#Creating DataFrame\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "india = pd.DataFrame({'President Name': names,'Term':term})\n",
    "india\n",
    "\n",
    "\n",
    " \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b367ab24",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Headline</th>\n",
       "      <th>Time</th>\n",
       "      <th>Link</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>UBS says Generac is a top pick with more than ...</td>\n",
       "      <td>32 Min</td>\n",
       "      <td>https://www.cnbc.com/2022/06/02/ubs-says-gener...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Walmart bets its stores will give it an edge i...</td>\n",
       "      <td>45 Min</td>\n",
       "      <td>https://www.cnbc.com/2022/06/02/walmart-bets-i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5 things to know before the stock market opens...</td>\n",
       "      <td>47 Min</td>\n",
       "      <td>https://www.cnbc.com/2022/06/02/5-things-to-kn...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>D.A. Davidson sees more downside for Rivian sh...</td>\n",
       "      <td>1 Hour</td>\n",
       "      <td>https://www.cnbc.com/2022/06/02/da-davidson-se...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Piper Sandler downgrades Pinterest and Snap, s...</td>\n",
       "      <td>2 Hours</td>\n",
       "      <td>https://www.cnbc.com/2022/06/02/piper-sandler-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Russia takes majority control of key Ukrainian...</td>\n",
       "      <td>2 Hours</td>\n",
       "      <td>https://www.cnbc.com/2022/06/02/russia-ukraine...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>U.S. Treasury yields dip ahead of employment data</td>\n",
       "      <td>4 Hours</td>\n",
       "      <td>https://www.cnbc.com/2022/06/02/us-treasury-yi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>TerraUSD collapse will 'probably be the end' o...</td>\n",
       "      <td>5 Hours</td>\n",
       "      <td>https://www.cnbc.com/2022/06/02/ust-debacle-wi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Ideas about recycling and nature are changing ...</td>\n",
       "      <td>5 Hours</td>\n",
       "      <td>https://www.cnbc.com/2022/06/02/circular-econo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>European stocks rise as traders assess data, O...</td>\n",
       "      <td>5 Hours</td>\n",
       "      <td>https://www.cnbc.com/2022/06/02/european-stock...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>How Russia could try to get around the Europea...</td>\n",
       "      <td>7 Hours</td>\n",
       "      <td>https://www.cnbc.com/2022/06/02/how-russia-cou...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>First time on a yacht? Avoid these 7 amateur m...</td>\n",
       "      <td>8 Hours</td>\n",
       "      <td>https://www.cnbc.com/2022/06/02/first-time-on-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Oil slides after report Saudi Arabia could ste...</td>\n",
       "      <td>9 Hours</td>\n",
       "      <td>https://www.cnbc.com/2022/06/02/oil-prices-eu-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Solana suffered its second outage in a month, ...</td>\n",
       "      <td>10 Hours</td>\n",
       "      <td>https://www.cnbc.com/2022/06/01/solana-suffere...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Goldman names its top battery stock to play ri...</td>\n",
       "      <td>10 Hours</td>\n",
       "      <td>https://www.cnbc.com/2022/06/02/goldman-sachs-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>'Fallen angels': Morgan Stanley says buy the d...</td>\n",
       "      <td>11 Hours</td>\n",
       "      <td>https://www.cnbc.com/2022/06/02/morgan-stanley...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Self-driving car companies' first step to maki...</td>\n",
       "      <td>11 Hours</td>\n",
       "      <td>https://www.cnbc.com/2022/06/02/self-driving-c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Javier Olivan, who's replacing Sandberg at Met...</td>\n",
       "      <td>11 Hours</td>\n",
       "      <td>https://www.cnbc.com/2022/06/01/javier-olivan-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Sandberg was Facebook's adult, but it's always...</td>\n",
       "      <td>12 Hours</td>\n",
       "      <td>https://www.cnbc.com/2022/06/01/sheryl-sandber...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Asia-Pacific stocks struggle for direction; oi...</td>\n",
       "      <td>12 Hours</td>\n",
       "      <td>https://www.cnbc.com/2022/06/02/asia-markets-a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>Cramer's lightning round: I like P&amp;G over Olaplex</td>\n",
       "      <td>13 Hours</td>\n",
       "      <td>https://www.cnbc.com/2022/06/01/cramers-lightn...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>Biden administration will cancel student debt ...</td>\n",
       "      <td>13 Hours</td>\n",
       "      <td>https://www.cnbc.com/2022/06/01/biden-administ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>Jim Cramer likes this alternative energy play ...</td>\n",
       "      <td>13 Hours</td>\n",
       "      <td>https://www.cnbc.com/2022/06/01/cramer-the-alt...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>Jim Cramer says investors should consider 3 th...</td>\n",
       "      <td>13 Hours</td>\n",
       "      <td>https://www.cnbc.com/2022/06/01/cramer-says-in...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>Stock futures rebound on Thursday as oil price...</td>\n",
       "      <td>14 Hours</td>\n",
       "      <td>https://www.cnbc.com/2022/06/01/stock-futures-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>Best trades on CNBC Wednesday: Safety stocks a...</td>\n",
       "      <td>14 Hours</td>\n",
       "      <td>https://www.cnbc.com/2022/06/01/best-trades-on...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>Reckitt says it's ready to import 21 million b...</td>\n",
       "      <td>14 Hours</td>\n",
       "      <td>https://www.cnbc.com/2022/06/01/reckitt-baby-f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>Why Meta drop on Sandberg's exit is a good cha...</td>\n",
       "      <td>15 Hours</td>\n",
       "      <td>https://www.cnbc.com/2022/06/01/why-drop-in-me...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>Stocks making the biggest moves after hours: C...</td>\n",
       "      <td>15 Hours</td>\n",
       "      <td>https://www.cnbc.com/2022/06/01/stocks-making-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>Tough new sanctions on Russia could change the...</td>\n",
       "      <td>15 Hours</td>\n",
       "      <td>https://www.cnbc.com/2022/06/01/tough-new-sanc...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             Headline       Time  \\\n",
       "0   UBS says Generac is a top pick with more than ...    32 Min    \n",
       "1   Walmart bets its stores will give it an edge i...    45 Min    \n",
       "2   5 things to know before the stock market opens...    47 Min    \n",
       "3   D.A. Davidson sees more downside for Rivian sh...    1 Hour    \n",
       "4   Piper Sandler downgrades Pinterest and Snap, s...   2 Hours    \n",
       "5   Russia takes majority control of key Ukrainian...   2 Hours    \n",
       "6   U.S. Treasury yields dip ahead of employment data   4 Hours    \n",
       "7   TerraUSD collapse will 'probably be the end' o...   5 Hours    \n",
       "8   Ideas about recycling and nature are changing ...   5 Hours    \n",
       "9   European stocks rise as traders assess data, O...   5 Hours    \n",
       "10  How Russia could try to get around the Europea...   7 Hours    \n",
       "11  First time on a yacht? Avoid these 7 amateur m...   8 Hours    \n",
       "12  Oil slides after report Saudi Arabia could ste...   9 Hours    \n",
       "13  Solana suffered its second outage in a month, ...  10 Hours    \n",
       "14  Goldman names its top battery stock to play ri...  10 Hours    \n",
       "15  'Fallen angels': Morgan Stanley says buy the d...  11 Hours    \n",
       "16  Self-driving car companies' first step to maki...  11 Hours    \n",
       "17  Javier Olivan, who's replacing Sandberg at Met...  11 Hours    \n",
       "18  Sandberg was Facebook's adult, but it's always...  12 Hours    \n",
       "19  Asia-Pacific stocks struggle for direction; oi...  12 Hours    \n",
       "20  Cramer's lightning round: I like P&G over Olaplex  13 Hours    \n",
       "21  Biden administration will cancel student debt ...  13 Hours    \n",
       "22  Jim Cramer likes this alternative energy play ...  13 Hours    \n",
       "23  Jim Cramer says investors should consider 3 th...  13 Hours    \n",
       "24  Stock futures rebound on Thursday as oil price...  14 Hours    \n",
       "25  Best trades on CNBC Wednesday: Safety stocks a...  14 Hours    \n",
       "26  Reckitt says it's ready to import 21 million b...  14 Hours    \n",
       "27  Why Meta drop on Sandberg's exit is a good cha...  15 Hours    \n",
       "28  Stocks making the biggest moves after hours: C...  15 Hours    \n",
       "29  Tough new sanctions on Russia could change the...  15 Hours    \n",
       "\n",
       "                                                 Link  \n",
       "0   https://www.cnbc.com/2022/06/02/ubs-says-gener...  \n",
       "1   https://www.cnbc.com/2022/06/02/walmart-bets-i...  \n",
       "2   https://www.cnbc.com/2022/06/02/5-things-to-kn...  \n",
       "3   https://www.cnbc.com/2022/06/02/da-davidson-se...  \n",
       "4   https://www.cnbc.com/2022/06/02/piper-sandler-...  \n",
       "5   https://www.cnbc.com/2022/06/02/russia-ukraine...  \n",
       "6   https://www.cnbc.com/2022/06/02/us-treasury-yi...  \n",
       "7   https://www.cnbc.com/2022/06/02/ust-debacle-wi...  \n",
       "8   https://www.cnbc.com/2022/06/02/circular-econo...  \n",
       "9   https://www.cnbc.com/2022/06/02/european-stock...  \n",
       "10  https://www.cnbc.com/2022/06/02/how-russia-cou...  \n",
       "11  https://www.cnbc.com/2022/06/02/first-time-on-...  \n",
       "12  https://www.cnbc.com/2022/06/02/oil-prices-eu-...  \n",
       "13  https://www.cnbc.com/2022/06/01/solana-suffere...  \n",
       "14  https://www.cnbc.com/2022/06/02/goldman-sachs-...  \n",
       "15  https://www.cnbc.com/2022/06/02/morgan-stanley...  \n",
       "16  https://www.cnbc.com/2022/06/02/self-driving-c...  \n",
       "17  https://www.cnbc.com/2022/06/01/javier-olivan-...  \n",
       "18  https://www.cnbc.com/2022/06/01/sheryl-sandber...  \n",
       "19  https://www.cnbc.com/2022/06/02/asia-markets-a...  \n",
       "20  https://www.cnbc.com/2022/06/01/cramers-lightn...  \n",
       "21  https://www.cnbc.com/2022/06/01/biden-administ...  \n",
       "22  https://www.cnbc.com/2022/06/01/cramer-the-alt...  \n",
       "23  https://www.cnbc.com/2022/06/01/cramer-says-in...  \n",
       "24  https://www.cnbc.com/2022/06/01/stock-futures-...  \n",
       "25  https://www.cnbc.com/2022/06/01/best-trades-on...  \n",
       "26  https://www.cnbc.com/2022/06/01/reckitt-baby-f...  \n",
       "27  https://www.cnbc.com/2022/06/01/why-drop-in-me...  \n",
       "28  https://www.cnbc.com/2022/06/01/stocks-making-...  \n",
       "29  https://www.cnbc.com/2022/06/01/tough-new-sanc...  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# # Ques 7: Write a python program to scrape mentioned news details from https://www.cnbc.com/world/?region=world :\n",
    "# ## i) Headline\n",
    "# ## ii) Time\n",
    "# ## iii) News Link\n",
    "\n",
    "\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "\n",
    "\n",
    "\n",
    "page=requests.get('https://www.cnbc.com/world/?region=world')\n",
    "\n",
    "\n",
    "\n",
    "page\n",
    "\n",
    "\n",
    "\n",
    "soup=BeautifulSoup(page.content, 'html.parser')\n",
    "soup\n",
    "\n",
    "\n",
    "\n",
    "headline=[]\n",
    "\n",
    "for i in soup.find_all('div', class_=\"LatestNews-headlineWrapper\"):\n",
    "    headline.append(i.text.split(sep='Ago')[1]) \n",
    "    \n",
    "headline\n",
    "\n",
    "\n",
    "time=[]\n",
    "\n",
    "for i in soup.find_all('div', class_=\"LatestNews-headlineWrapper\"):\n",
    "    time.append(i.text.split(sep='Ago')[0]) \n",
    "    \n",
    "time\n",
    "\n",
    "\n",
    "\n",
    "headline_link=[]\n",
    "\n",
    "for link in soup.find_all('a', class_=\"LatestNews-headline\"):\n",
    "    headline_link.append(link.get('href'))\n",
    "\n",
    "    \n",
    "#Creating DataFrame\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "news = pd.DataFrame({'Headline': headline,'Time':time, 'Link': headline_link})\n",
    "news\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "89807fb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25 25 25 25\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Title</th>\n",
       "      <th>Author</th>\n",
       "      <th>Published Date</th>\n",
       "      <th>URL</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Reward is enough</td>\n",
       "      <td>Silver, David, Singh, Satinder, Precup, Doina,...</td>\n",
       "      <td>October 2021</td>\n",
       "      <td>https://www.sciencedirect.com/science/article/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Making sense of raw input</td>\n",
       "      <td>Evans, Richard, Bošnjak, Matko and 5 more</td>\n",
       "      <td>October 2021</td>\n",
       "      <td>https://www.sciencedirect.com/science/article/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Law and logic: A review from an argumentation ...</td>\n",
       "      <td>Prakken, Henry, Sartor, Giovanni</td>\n",
       "      <td>October 2015</td>\n",
       "      <td>https://www.sciencedirect.com/science/article/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Creativity and artificial intelligence</td>\n",
       "      <td>Boden, Margaret A.</td>\n",
       "      <td>August 1998</td>\n",
       "      <td>https://www.sciencedirect.com/science/article/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Artificial cognition for social human–robot in...</td>\n",
       "      <td>Lemaignan, Séverin, Warnier, Mathieu and 3 more</td>\n",
       "      <td>June 2017</td>\n",
       "      <td>https://www.sciencedirect.com/science/article/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Explanation in artificial intelligence: Insigh...</td>\n",
       "      <td>Miller, Tim</td>\n",
       "      <td>February 2019</td>\n",
       "      <td>https://www.sciencedirect.com/science/article/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Making sense of sensory input</td>\n",
       "      <td>Evans, Richard, Hernández-Orallo, José and 3 more</td>\n",
       "      <td>April 2021</td>\n",
       "      <td>https://www.sciencedirect.com/science/article/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Conflict-based search for optimal multi-agent ...</td>\n",
       "      <td>Sharon, Guni, Stern, Roni, Felner, Ariel, Stur...</td>\n",
       "      <td>February 2015</td>\n",
       "      <td>https://www.sciencedirect.com/science/article/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Between MDPs and semi-MDPs: A framework for te...</td>\n",
       "      <td>Sutton, Richard S., Precup, Doina, Singh, Sati...</td>\n",
       "      <td>August 1999</td>\n",
       "      <td>https://www.sciencedirect.com/science/article/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>The Hanabi challenge: A new frontier for AI re...</td>\n",
       "      <td>Bard, Nolan, Foerster, Jakob N. and 13 more</td>\n",
       "      <td>March 2020</td>\n",
       "      <td>https://www.sciencedirect.com/science/article/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Evaluating XAI: A comparison of rule-based and...</td>\n",
       "      <td>van der Waa, Jasper, Nieuwburg, Elisabeth, Cre...</td>\n",
       "      <td>February 2021</td>\n",
       "      <td>https://www.sciencedirect.com/science/article/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Argumentation in artificial intelligence</td>\n",
       "      <td>Bench-Capon, T.J.M., Dunne, Paul E.</td>\n",
       "      <td>October 2007</td>\n",
       "      <td>https://www.sciencedirect.com/science/article/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Algorithms for computing strategies in two-pla...</td>\n",
       "      <td>Bošanský, Branislav, Lisý, Viliam and 3 more</td>\n",
       "      <td>August 2016</td>\n",
       "      <td>https://www.sciencedirect.com/science/article/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Multiple object tracking: A literature review</td>\n",
       "      <td>Luo, Wenhan, Xing, Junliang and 4 more</td>\n",
       "      <td>April 2021</td>\n",
       "      <td>https://www.sciencedirect.com/science/article/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Selection of relevant features and examples in...</td>\n",
       "      <td>Blum, Avrim L., Langley, Pat</td>\n",
       "      <td>December 1997</td>\n",
       "      <td>https://www.sciencedirect.com/science/article/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>A survey of inverse reinforcement learning: Ch...</td>\n",
       "      <td>Arora, Saurabh, Doshi, Prashant</td>\n",
       "      <td>August 2021</td>\n",
       "      <td>https://www.sciencedirect.com/science/article/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Explaining individual predictions when feature...</td>\n",
       "      <td>Aas, Kjersti, Jullum, Martin, Løland, Anders</td>\n",
       "      <td>September 2021</td>\n",
       "      <td>https://www.sciencedirect.com/science/article/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>A review of possible effects of cognitive bias...</td>\n",
       "      <td>Kliegr, Tomáš, Bahník, Štěpán, Fürnkranz, Joha...</td>\n",
       "      <td>June 2021</td>\n",
       "      <td>https://www.sciencedirect.com/science/article/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Integrating social power into the decision-mak...</td>\n",
       "      <td>Pereira, Gonçalo, Prada, Rui, Santos, Pedro A.</td>\n",
       "      <td>December 2016</td>\n",
       "      <td>https://www.sciencedirect.com/science/article/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>“That's (not) the output I expected!” On the r...</td>\n",
       "      <td>Riveiro, Maria, Thill, Serge</td>\n",
       "      <td>September 2021</td>\n",
       "      <td>https://www.sciencedirect.com/science/article/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>Explaining black-box classifiers using post-ho...</td>\n",
       "      <td>Kenny, Eoin M., Ford, Courtney, Quinn, Molly, ...</td>\n",
       "      <td>May 2021</td>\n",
       "      <td>https://www.sciencedirect.com/science/article/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>Algorithm runtime prediction: Methods &amp; evalua...</td>\n",
       "      <td>Hutter, Frank, Xu, Lin, Hoos, Holger H., Leyto...</td>\n",
       "      <td>January 2014</td>\n",
       "      <td>https://www.sciencedirect.com/science/article/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>Wrappers for feature subset selection</td>\n",
       "      <td>Kohavi, Ron, John, George H.</td>\n",
       "      <td>December 1997</td>\n",
       "      <td>https://www.sciencedirect.com/science/article/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>Commonsense visual sensemaking for autonomous ...</td>\n",
       "      <td>Suchan, Jakob, Bhatt, Mehul, Varadarajan, Srik...</td>\n",
       "      <td>October 2021</td>\n",
       "      <td>https://www.sciencedirect.com/science/article/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>Quantum computation, quantum theory and AI</td>\n",
       "      <td>Ying, Mingsheng</td>\n",
       "      <td>February 2010</td>\n",
       "      <td>https://www.sciencedirect.com/science/article/...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Title  \\\n",
       "0                                    Reward is enough   \n",
       "1                           Making sense of raw input   \n",
       "2   Law and logic: A review from an argumentation ...   \n",
       "3              Creativity and artificial intelligence   \n",
       "4   Artificial cognition for social human–robot in...   \n",
       "5   Explanation in artificial intelligence: Insigh...   \n",
       "6                       Making sense of sensory input   \n",
       "7   Conflict-based search for optimal multi-agent ...   \n",
       "8   Between MDPs and semi-MDPs: A framework for te...   \n",
       "9   The Hanabi challenge: A new frontier for AI re...   \n",
       "10  Evaluating XAI: A comparison of rule-based and...   \n",
       "11           Argumentation in artificial intelligence   \n",
       "12  Algorithms for computing strategies in two-pla...   \n",
       "13      Multiple object tracking: A literature review   \n",
       "14  Selection of relevant features and examples in...   \n",
       "15  A survey of inverse reinforcement learning: Ch...   \n",
       "16  Explaining individual predictions when feature...   \n",
       "17  A review of possible effects of cognitive bias...   \n",
       "18  Integrating social power into the decision-mak...   \n",
       "19  “That's (not) the output I expected!” On the r...   \n",
       "20  Explaining black-box classifiers using post-ho...   \n",
       "21  Algorithm runtime prediction: Methods & evalua...   \n",
       "22              Wrappers for feature subset selection   \n",
       "23  Commonsense visual sensemaking for autonomous ...   \n",
       "24         Quantum computation, quantum theory and AI   \n",
       "\n",
       "                                               Author  Published Date  \\\n",
       "0   Silver, David, Singh, Satinder, Precup, Doina,...    October 2021   \n",
       "1           Evans, Richard, Bošnjak, Matko and 5 more    October 2021   \n",
       "2                   Prakken, Henry, Sartor, Giovanni     October 2015   \n",
       "3                                 Boden, Margaret A.      August 1998   \n",
       "4     Lemaignan, Séverin, Warnier, Mathieu and 3 more       June 2017   \n",
       "5                                        Miller, Tim    February 2019   \n",
       "6   Evans, Richard, Hernández-Orallo, José and 3 more      April 2021   \n",
       "7   Sharon, Guni, Stern, Roni, Felner, Ariel, Stur...   February 2015   \n",
       "8   Sutton, Richard S., Precup, Doina, Singh, Sati...     August 1999   \n",
       "9         Bard, Nolan, Foerster, Jakob N. and 13 more      March 2020   \n",
       "10  van der Waa, Jasper, Nieuwburg, Elisabeth, Cre...   February 2021   \n",
       "11               Bench-Capon, T.J.M., Dunne, Paul E.     October 2007   \n",
       "12       Bošanský, Branislav, Lisý, Viliam and 3 more     August 2016   \n",
       "13             Luo, Wenhan, Xing, Junliang and 4 more      April 2021   \n",
       "14                      Blum, Avrim L., Langley, Pat    December 1997   \n",
       "15                   Arora, Saurabh, Doshi, Prashant      August 2021   \n",
       "16      Aas, Kjersti, Jullum, Martin, Løland, Anders   September 2021   \n",
       "17  Kliegr, Tomáš, Bahník, Štěpán, Fürnkranz, Joha...       June 2021   \n",
       "18    Pereira, Gonçalo, Prada, Rui, Santos, Pedro A.    December 2016   \n",
       "19                      Riveiro, Maria, Thill, Serge   September 2021   \n",
       "20  Kenny, Eoin M., Ford, Courtney, Quinn, Molly, ...        May 2021   \n",
       "21  Hutter, Frank, Xu, Lin, Hoos, Holger H., Leyto...    January 2014   \n",
       "22                      Kohavi, Ron, John, George H.    December 1997   \n",
       "23  Suchan, Jakob, Bhatt, Mehul, Varadarajan, Srik...    October 2021   \n",
       "24                                   Ying, Mingsheng    February 2010   \n",
       "\n",
       "                                                  URL  \n",
       "0   https://www.sciencedirect.com/science/article/...  \n",
       "1   https://www.sciencedirect.com/science/article/...  \n",
       "2   https://www.sciencedirect.com/science/article/...  \n",
       "3   https://www.sciencedirect.com/science/article/...  \n",
       "4   https://www.sciencedirect.com/science/article/...  \n",
       "5   https://www.sciencedirect.com/science/article/...  \n",
       "6   https://www.sciencedirect.com/science/article/...  \n",
       "7   https://www.sciencedirect.com/science/article/...  \n",
       "8   https://www.sciencedirect.com/science/article/...  \n",
       "9   https://www.sciencedirect.com/science/article/...  \n",
       "10  https://www.sciencedirect.com/science/article/...  \n",
       "11  https://www.sciencedirect.com/science/article/...  \n",
       "12  https://www.sciencedirect.com/science/article/...  \n",
       "13  https://www.sciencedirect.com/science/article/...  \n",
       "14  https://www.sciencedirect.com/science/article/...  \n",
       "15  https://www.sciencedirect.com/science/article/...  \n",
       "16  https://www.sciencedirect.com/science/article/...  \n",
       "17  https://www.sciencedirect.com/science/article/...  \n",
       "18  https://www.sciencedirect.com/science/article/...  \n",
       "19  https://www.sciencedirect.com/science/article/...  \n",
       "20  https://www.sciencedirect.com/science/article/...  \n",
       "21  https://www.sciencedirect.com/science/article/...  \n",
       "22  https://www.sciencedirect.com/science/article/...  \n",
       "23  https://www.sciencedirect.com/science/article/...  \n",
       "24  https://www.sciencedirect.com/science/article/...  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# # Ques 8:  Write a python program to scrape the details of most downloaded articles from AI in last 90 days. \n",
    "# ## https://www.journals.elsevier.com/artificial-intelligence/most-downloaded-articles\n",
    "# ## Scrape below mentioned details :\n",
    "# ## i) Paper Title \n",
    "# ## ii) Authors\n",
    "# ## iii) Published Date \n",
    "# ## iv) Paper URL\n",
    "\n",
    "\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "\n",
    "\n",
    "\n",
    "page=requests.get('https://www.journals.elsevier.com/artificial-intelligence/most-downloaded-articles')\n",
    "\n",
    "\n",
    "\n",
    "page\n",
    "\n",
    "\n",
    "\n",
    "soup=BeautifulSoup(page.content, 'html.parser')\n",
    "soup\n",
    "\n",
    "\n",
    "\n",
    "title=[]\n",
    "\n",
    "for i in soup.find_all('h2', class_=\"sc-1qrq3sd-1 MKjKb sc-1nmom32-0 sc-1nmom32-1 hqhUYH ebTA-dR\"):\n",
    "    title.append(i.text) \n",
    "    \n",
    "title\n",
    "\n",
    "\n",
    "\n",
    "author=[]\n",
    "\n",
    "for i in soup.find_all('span', class_=\"sc-1w3fpd7-0 pgLAT\"):\n",
    "    author.append(i.text) \n",
    "    \n",
    "author\n",
    "\n",
    "\n",
    "\n",
    "released=[]\n",
    "\n",
    "for i in soup.find_all('span', class_=\"sc-1thf9ly-2 bKddwo\"):\n",
    "    released.append(i.text) \n",
    "    \n",
    "released\n",
    "\n",
    "\n",
    "\n",
    "url = []\n",
    "\n",
    "for link in soup.find_all('a', class_=\"sc-5smygv-0 nrDZj\"):\n",
    "    url.append(link.get('href'))\n",
    "\n",
    "\n",
    "\n",
    "print(len(title),len(author), len(released),len(url))\n",
    "\n",
    "\n",
    "\n",
    "#Creating DataFrame\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "AI = pd.DataFrame({'Title': title,'Author':author,'Published Date':released,'URL':url})\n",
    "AI\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1efe5e1d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Restaurant name</th>\n",
       "      <th>Cuisine</th>\n",
       "      <th>Location</th>\n",
       "      <th>Ratings</th>\n",
       "      <th>Image URL</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Castle Barbeque</td>\n",
       "      <td>North Indian, Chinese</td>\n",
       "      <td>Connaught Place, Central Delhi</td>\n",
       "      <td>3.5</td>\n",
       "      <td>https://im1.dineout.co.in/images/uploads/resta...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Jungle Jamboree</td>\n",
       "      <td>North Indian, Asian, Italian</td>\n",
       "      <td>3CS Mall,Lajpat Nagar - 3, South Delhi</td>\n",
       "      <td>3.9</td>\n",
       "      <td>https://im1.dineout.co.in/images/uploads/resta...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Castle Barbeque</td>\n",
       "      <td>Chinese, North Indian</td>\n",
       "      <td>Pacific Mall,Tagore Garden, West Delhi</td>\n",
       "      <td>3.9</td>\n",
       "      <td>https://im1.dineout.co.in/images/uploads/resta...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Cafe Knosh</td>\n",
       "      <td>Italian, Continental</td>\n",
       "      <td>The Leela Ambience Convention Hotel,Shahdara, ...</td>\n",
       "      <td>4.3</td>\n",
       "      <td>https://im1.dineout.co.in/images/uploads/resta...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The Barbeque Company</td>\n",
       "      <td>North Indian, Chinese</td>\n",
       "      <td>Gardens Galleria,Sector 38A, Noida</td>\n",
       "      <td>4</td>\n",
       "      <td>https://im1.dineout.co.in/images/uploads/resta...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>India Grill</td>\n",
       "      <td>North Indian, Italian</td>\n",
       "      <td>Hilton Garden Inn,Saket, South Delhi</td>\n",
       "      <td>3.9</td>\n",
       "      <td>https://im1.dineout.co.in/images/uploads/resta...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Delhi Barbeque</td>\n",
       "      <td>North Indian</td>\n",
       "      <td>Taurus Sarovar Portico,Mahipalpur, South Delhi</td>\n",
       "      <td>3.7</td>\n",
       "      <td>https://im1.dineout.co.in/images/uploads/resta...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>The Monarch - Bar Be Que Village</td>\n",
       "      <td>North Indian</td>\n",
       "      <td>Indirapuram Habitat Centre,Indirapuram, Ghaziabad</td>\n",
       "      <td>3.8</td>\n",
       "      <td>https://im1.dineout.co.in/images/uploads/resta...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>World Cafe</td>\n",
       "      <td>North Indian, Italian</td>\n",
       "      <td>Vibe by The Lalit Traveller,Sector 35, Faridabad</td>\n",
       "      <td>4.2</td>\n",
       "      <td>https://im1.dineout.co.in/images/uploads/resta...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Indian Grill Room</td>\n",
       "      <td>North Indian, Mughlai</td>\n",
       "      <td>Suncity Business Tower,Golf Course Road, Gurgaon</td>\n",
       "      <td>4.3</td>\n",
       "      <td>https://im1.dineout.co.in/images/uploads/resta...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Mad 4 Bar B Que</td>\n",
       "      <td>North Indian</td>\n",
       "      <td>Sector 29, Faridabad</td>\n",
       "      <td>3.6</td>\n",
       "      <td>https://im1.dineout.co.in/images/uploads/resta...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Barbeque 29</td>\n",
       "      <td>North Indian, Mughlai, Desserts, Beverages</td>\n",
       "      <td>NIT, Faridabad</td>\n",
       "      <td>4.2</td>\n",
       "      <td>https://im1.dineout.co.in/images/uploads/resta...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Glasshouse</td>\n",
       "      <td>European, Italian, Asian, Continental</td>\n",
       "      <td>DoubleTree By Hilton Gurugram Baani Square,Sec...</td>\n",
       "      <td>4</td>\n",
       "      <td>https://im1.dineout.co.in/images/uploads/resta...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     Restaurant name  \\\n",
       "0                    Castle Barbeque   \n",
       "1                    Jungle Jamboree   \n",
       "2                    Castle Barbeque   \n",
       "3                         Cafe Knosh   \n",
       "4               The Barbeque Company   \n",
       "5                        India Grill   \n",
       "6                     Delhi Barbeque   \n",
       "7   The Monarch - Bar Be Que Village   \n",
       "8                         World Cafe   \n",
       "9                  Indian Grill Room   \n",
       "10                   Mad 4 Bar B Que   \n",
       "11                       Barbeque 29   \n",
       "12                        Glasshouse   \n",
       "\n",
       "                                        Cuisine  \\\n",
       "0                         North Indian, Chinese   \n",
       "1                  North Indian, Asian, Italian   \n",
       "2                         Chinese, North Indian   \n",
       "3                          Italian, Continental   \n",
       "4                         North Indian, Chinese   \n",
       "5                         North Indian, Italian   \n",
       "6                                  North Indian   \n",
       "7                                  North Indian   \n",
       "8                         North Indian, Italian   \n",
       "9                         North Indian, Mughlai   \n",
       "10                                 North Indian   \n",
       "11   North Indian, Mughlai, Desserts, Beverages   \n",
       "12        European, Italian, Asian, Continental   \n",
       "\n",
       "                                            Location  Ratings  \\\n",
       "0                      Connaught Place, Central Delhi     3.5   \n",
       "1              3CS Mall,Lajpat Nagar - 3, South Delhi     3.9   \n",
       "2              Pacific Mall,Tagore Garden, West Delhi     3.9   \n",
       "3   The Leela Ambience Convention Hotel,Shahdara, ...     4.3   \n",
       "4                  Gardens Galleria,Sector 38A, Noida       4   \n",
       "5                Hilton Garden Inn,Saket, South Delhi     3.9   \n",
       "6      Taurus Sarovar Portico,Mahipalpur, South Delhi     3.7   \n",
       "7   Indirapuram Habitat Centre,Indirapuram, Ghaziabad     3.8   \n",
       "8    Vibe by The Lalit Traveller,Sector 35, Faridabad     4.2   \n",
       "9    Suncity Business Tower,Golf Course Road, Gurgaon     4.3   \n",
       "10                               Sector 29, Faridabad     3.6   \n",
       "11                                     NIT, Faridabad     4.2   \n",
       "12  DoubleTree By Hilton Gurugram Baani Square,Sec...       4   \n",
       "\n",
       "                                            Image URL  \n",
       "0   https://im1.dineout.co.in/images/uploads/resta...  \n",
       "1   https://im1.dineout.co.in/images/uploads/resta...  \n",
       "2   https://im1.dineout.co.in/images/uploads/resta...  \n",
       "3   https://im1.dineout.co.in/images/uploads/resta...  \n",
       "4   https://im1.dineout.co.in/images/uploads/resta...  \n",
       "5   https://im1.dineout.co.in/images/uploads/resta...  \n",
       "6   https://im1.dineout.co.in/images/uploads/resta...  \n",
       "7   https://im1.dineout.co.in/images/uploads/resta...  \n",
       "8   https://im1.dineout.co.in/images/uploads/resta...  \n",
       "9   https://im1.dineout.co.in/images/uploads/resta...  \n",
       "10  https://im1.dineout.co.in/images/uploads/resta...  \n",
       "11  https://im1.dineout.co.in/images/uploads/resta...  \n",
       "12  https://im1.dineout.co.in/images/uploads/resta...  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# # Ques 9: Write a python program to scrape mentioned details from dineout.co.in :\n",
    "# ## i) Restaurant name\n",
    "# ## ii) Cuisine\n",
    "# ## iii) Location \n",
    "# ## iv) Ratings\n",
    "# ## v) Image URL\n",
    "\n",
    "\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "\n",
    "\n",
    "\n",
    "page=requests.get('https://www.dineout.co.in/delhi-restaurants/buffet-special')\n",
    "\n",
    "\n",
    "\n",
    "page\n",
    "\n",
    "\n",
    "\n",
    "soup=BeautifulSoup(page.content, 'html.parser')\n",
    "soup\n",
    "\n",
    "\n",
    "\n",
    "restaurant_name=[]\n",
    "\n",
    "for i in soup.find_all('a', class_=\"restnt-name ellipsis\"):\n",
    "    restaurant_name.append(i.text) \n",
    "    \n",
    "restaurant_name\n",
    "\n",
    "\n",
    "\n",
    "cuisine=[]\n",
    "\n",
    "for i in soup.find_all('span', class_=\"double-line-ellipsis\"):\n",
    "    cuisine.append(i.text.split(sep='|')[1]) \n",
    "    \n",
    "cuisine\n",
    "\n",
    "\n",
    "\n",
    "loc=[]\n",
    "\n",
    "for i in soup.find_all('div', class_=\"restnt-loc ellipsis\"):\n",
    "    loc.append(i.text) \n",
    "    \n",
    "loc\n",
    "\n",
    "\n",
    "\n",
    "ratings=[]\n",
    "for i in soup.find_all('div', class_=\"restnt-rating rating-4\"):\n",
    "    ratings.append(i.text) \n",
    "    \n",
    "ratings\n",
    "\n",
    "\n",
    "\n",
    "img_url=[]\n",
    "\n",
    "for i in soup.find_all('img',class_=\"no-img\"):\n",
    "    img_url.append(i['data-src'])\n",
    "    \n",
    "img_url\n",
    "\n",
    "\n",
    "\n",
    "#Creating DataFrame\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "dineout = pd.DataFrame({'Restaurant name': restaurant_name,'Cuisine': cuisine, 'Location ': loc, 'Ratings': ratings, 'Image URL': img_url})\n",
    "dineout\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "90a0a568",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 100 100 100\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Rank</th>\n",
       "      <th>Publication</th>\n",
       "      <th>H5-index</th>\n",
       "      <th>H5-median</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.</td>\n",
       "      <td>Nature</td>\n",
       "      <td>[414]</td>\n",
       "      <td>607</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2.</td>\n",
       "      <td>The New England Journal of Medicine</td>\n",
       "      <td>[410]</td>\n",
       "      <td>704</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3.</td>\n",
       "      <td>Science</td>\n",
       "      <td>[391]</td>\n",
       "      <td>564</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.</td>\n",
       "      <td>IEEE/CVF Conference on Computer Vision and Pat...</td>\n",
       "      <td>[356]</td>\n",
       "      <td>583</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.</td>\n",
       "      <td>The Lancet</td>\n",
       "      <td>[345]</td>\n",
       "      <td>600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>96.</td>\n",
       "      <td>Frontiers in Immunology</td>\n",
       "      <td>[134]</td>\n",
       "      <td>177</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>97.</td>\n",
       "      <td>Small</td>\n",
       "      <td>[134]</td>\n",
       "      <td>173</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>98.</td>\n",
       "      <td>Nature Immunology</td>\n",
       "      <td>[133]</td>\n",
       "      <td>210</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>99.</td>\n",
       "      <td>JAMA Oncology</td>\n",
       "      <td>[133]</td>\n",
       "      <td>202</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>100.</td>\n",
       "      <td>The Lancet Neurology</td>\n",
       "      <td>[133]</td>\n",
       "      <td>200</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    Rank                                        Publication  H5-index  \\\n",
       "0     1.                                             Nature     [414]   \n",
       "1     2.                The New England Journal of Medicine     [410]   \n",
       "2     3.                                            Science     [391]   \n",
       "3     4.  IEEE/CVF Conference on Computer Vision and Pat...     [356]   \n",
       "4     5.                                         The Lancet     [345]   \n",
       "..   ...                                                ...       ...   \n",
       "95   96.                            Frontiers in Immunology     [134]   \n",
       "96   97.                                              Small     [134]   \n",
       "97   98.                                  Nature Immunology     [133]   \n",
       "98   99.                                      JAMA Oncology     [133]   \n",
       "99  100.                               The Lancet Neurology     [133]   \n",
       "\n",
       "    H5-median  \n",
       "0         607  \n",
       "1         704  \n",
       "2         564  \n",
       "3         583  \n",
       "4         600  \n",
       "..        ...  \n",
       "95        177  \n",
       "96        173  \n",
       "97        210  \n",
       "98        202  \n",
       "99        200  \n",
       "\n",
       "[100 rows x 4 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# # Ques 10: Write a python program to scrape the details of top publications from Google Scholar from https://scholar.google.com/citations?view_op=top_venues&hl=en\n",
    "# ## i) Rank \n",
    "# ## ii) Publication\n",
    "# ## iii) h5-index\n",
    "# ## iv) h5-median\n",
    "\n",
    "\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "\n",
    "\n",
    "\n",
    "page=requests.get('https://scholar.google.com/citations?view_op=top_venues&hl=en')\n",
    "page\n",
    "\n",
    "\n",
    "\n",
    "soup=BeautifulSoup(page.content, 'html.parser')\n",
    "soup\n",
    "\n",
    "\n",
    "\n",
    "rank=[]\n",
    "for i in soup.find_all('td', class_=\"gsc_mvt_p\"):\n",
    "    rank.append(i.text) \n",
    "    \n",
    "rank\n",
    "\n",
    "\n",
    "\n",
    "publication=[]\n",
    "for i in soup.find_all('td', class_=\"gsc_mvt_t\"):\n",
    "    publication.append(i.text) \n",
    "    \n",
    "publication\n",
    "\n",
    "\n",
    "\n",
    "h5i=[]\n",
    "for i in soup.find_all('a', class_=\"gs_ibl gsc_mp_anchor\"):\n",
    "    h5i.append(i.text.split()) \n",
    "    \n",
    "h5i\n",
    "\n",
    "\n",
    "\n",
    "h5m=[]\n",
    "for i in soup.find_all('span', class_=\"gs_ibl gsc_mp_anchor\"):\n",
    "    h5m.append(i.text) \n",
    "    \n",
    "h5m\n",
    "\n",
    "\n",
    "\n",
    "print(len(rank),len(publication),len(h5i),len(h5m))\n",
    "\n",
    "\n",
    "\n",
    "#Creating DataFrame\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "top_pub = pd.DataFrame({'Rank': rank,'Publication':publication, ' H5-index': h5i, ' H5-median': h5m})\n",
    "top_pub\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30da035a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
